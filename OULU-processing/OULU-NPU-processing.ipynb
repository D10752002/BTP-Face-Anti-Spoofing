{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install mtcnn\n",
    "# !pip install tensorflow\n",
    "# !pip install facenet-pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing OULU-NPU into image frames for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import shutil\n",
    "import random\n",
    "import string\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "# from imutils import face_utils\n",
    "import matplotlib.pyplot as plt\n",
    "# from mtcnn.mtcnn import MTCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# face_cascade = cv2.CascadeClassifier('./haarcascade_frontalface_default.xml')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_face(img):\n",
    "#     # Convert to grayscale \n",
    "#     gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)  \n",
    "  \n",
    "#     # Detect the faces  \n",
    "#     faces = face_cascade.detectMultiScale(gray, 1.3, 4)  \n",
    "    \n",
    "#     # Draw rectangle around the faces and crop the faces\n",
    "#     for (x, y, w, h) in faces:\n",
    "#         faces = img[y:y + h, x:x + w]\n",
    "        \n",
    "#     return faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_face(img):\n",
    "#     # Detect faces\n",
    "#     results = detector.detect_faces(img)\n",
    "    \n",
    "#     faces = []\n",
    "#     for result in results:\n",
    "#         x, y, w, h = result['box']\n",
    "#         face = img[y:y+h, x:x+w]\n",
    "#         faces.append(face)\n",
    "    \n",
    "#     return faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_face(img):\n",
    "#     face_cascade = cv2.CascadeClassifier('./haarcascade_frontalface_default.xml')  \n",
    "#     gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)  \n",
    "#     faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "    \n",
    "#     face_images = []\n",
    "#     for (x, y, w, h) in faces:\n",
    "#         face = img[y:y+h, x:x+w]\n",
    "#         face_images.append(face)\n",
    "    \n",
    "#     return face_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_face_and_save(image_path, output_path):\n",
    "#     # Load the image\n",
    "#     img = cv2.imread(image_path)\n",
    "    \n",
    "#     # Convert to grayscale\n",
    "#     gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "  \n",
    "#     # Detect the faces\n",
    "#     faces = face_cascade.detectMultiScale(gray, 1.3, 4)\n",
    "    \n",
    "#     # Draw rectangle around the faces and save the image\n",
    "#     for (x, y, w, h) in faces:\n",
    "#         cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)  # Draw a green rectangle around the face\n",
    "        \n",
    "#     # Save the image with rectangles\n",
    "#     cv2.imwrite(output_path, img)\n",
    "    \n",
    "#     return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_face_and_save(image_path, output_path):\n",
    "#     # Load the image\n",
    "#     img = cv2.imread(image_path)\n",
    "    \n",
    "#     # Initialize the MTCNN detector\n",
    "#     detector = MTCNN()\n",
    "    \n",
    "#     # Detect faces\n",
    "#     results = detector.detect_faces(img)\n",
    "    \n",
    "#     # Draw rectangle around the faces and save the image\n",
    "#     for result in results:\n",
    "#         x, y, w, h = result['box']\n",
    "#         cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)  # Draw a green rectangle around the face\n",
    "    \n",
    "#     # Save the image with rectangles\n",
    "#     cv2.imwrite(output_path, img)\n",
    "    \n",
    "#     return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract_face_and_save('./test_img.jpg', 'test_img_output_face_bound.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_cascade = cv2.CascadeClassifier('./haarcascade_frontalface_default.xml')\n",
    "\n",
    "def extract_face(img, eye_coords):\n",
    "    # Get eye coordinates\n",
    "    x_eye_left, y_eye_left, x_eye_right, y_eye_right = map(int, eye_coords)\n",
    "    # print(x_eye_left, y_eye_left, x_eye_right, y_eye_right)\n",
    "    # Calculate the approximate size of the face using the distance between the eyes\n",
    "    face_width = abs(x_eye_left - x_eye_right) * 3.5  # Adjust factor as needed\n",
    "    face_height = face_width * 1.2  # Adjust factor as needed\n",
    "\n",
    "    # Calculate the center point between the eyes\n",
    "    center_x = (x_eye_left + x_eye_right) // 2\n",
    "    center_y = (y_eye_left + y_eye_right) // 2\n",
    "    # Define bounding box around the face\n",
    "    # print(int(center_x - face_width // 2), int(center_x + face_width // 2))\n",
    "    # print(int(center_y - face_height // 2), int(center_y + face_height // 2))\n",
    "    \n",
    "    x1 = max(0, int(center_x - face_width // 2))\n",
    "    y1 = max(0, int(center_y - face_height // 2))\n",
    "    x2 = min(img.shape[1], int(center_x + face_width // 2))\n",
    "    y2 = min(img.shape[0], int(center_y + face_height // 2))\n",
    "    if x1 < x2 and y1 < y2:\n",
    "        # Get the region around the eyes\n",
    "        # print(x1, y1, x2, y2)\n",
    "        eyes_region = img[y1:y2, x1:x2]\n",
    "        # Detect faces in the eyes region\n",
    "        gray_eyes = cv2.cvtColor(eyes_region, cv2.COLOR_BGR2GRAY)\n",
    "        def try_detect(scale_factor, min_neighbors):\n",
    "            try:\n",
    "                faces = face_cascade.detectMultiScale(gray_eyes, scale_factor, min_neighbors)\n",
    "                if len(faces) > 0:\n",
    "                    (fx, fy, fw, fh) = faces[0]\n",
    "\n",
    "                    fx1 = x1 + fx\n",
    "                    fy1 = y1 + fy\n",
    "                    fx2 = fx1 + fw\n",
    "                    fy2 = fy1 + fh\n",
    "\n",
    "                    face = img[fy1:fy2, fx1:fx2]\n",
    "\n",
    "                    return face\n",
    "\n",
    "            except Exception as e:\n",
    "                pass\n",
    "\n",
    "        # Try detecting with different parameters\n",
    "        face = try_detect(1.2, 3)\n",
    "        if face is None or face.shape[0] * face.shape[1] < 1e5:\n",
    "            face = try_detect(1.1, 5)\n",
    "        if face is None or face.shape[0] * face.shape[1] < 1e5:\n",
    "            face = try_detect(1.1, 3)\n",
    "        if face is None or face.shape[0] * face.shape[1] < 1e5:\n",
    "            face = try_detect(1.3, 4)\n",
    "        if face is None or face.shape[0] * face.shape[1] < 1e5:\n",
    "            face = try_detect(1.5, 4)\n",
    "        if face is not None:\n",
    "            return face\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "def read_eye_coordinates(txt_file):\n",
    "    with open(txt_file, 'r') as file:\n",
    "        eye_coords = [line.strip().split(',') for line in file]\n",
    "\n",
    "    return eye_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_multiple_videos(input_filenames, output_path, resize):\n",
    "    i = 0  # Counter of first video\n",
    "    abnormal_total_count=0\n",
    "    no_detections_total_count=0\n",
    "    for input_filename in tqdm(input_filenames, desc='Processing Videos'):\n",
    "        # print(f\"\\rFile: {i}\")\n",
    "        img_name = 'client'\n",
    "        pattern = input_filename.split('/')\n",
    "        last_substring = pattern[-1]  # Get the last substring after splitting by '/'\n",
    "        # print(last_substring)\n",
    "        f_name = last_substring.split('_')  # Split the last substring by underscores\n",
    "        if len(f_name) >= 4:  # Ensure there are at least 4 parts in the split\n",
    "            img_name = '_'.join(f_name[:3])  # Join the first three parts with underscores\n",
    "\n",
    "        # print(img_name)  # Print or use img_name as needed\n",
    "\n",
    "        # Get the eye coordinates for this video\n",
    "        eye_coordinates = read_eye_coordinates(input_filename.replace('.avi', '.txt'))\n",
    "\n",
    "        abnormal_count=0\n",
    "        no_detection_count=0\n",
    "        # Open video capture\n",
    "        cap = cv2.VideoCapture(input_filename)\n",
    "        sec_count = 0\n",
    "\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "\n",
    "            if ret:\n",
    "                try:\n",
    "                    eye_coords = eye_coordinates[sec_count]\n",
    "                    cropped_faces = extract_face(frame, eye_coords[1:])  # Exclude num_frame\n",
    "                    if cropped_faces is None:\n",
    "                        # print('No detection')\n",
    "                        sec_count +=1\n",
    "                        no_detection_count +=1\n",
    "                        continue\n",
    "                    if cropped_faces.shape[0]*cropped_faces.shape[1] < 1e5:\n",
    "                        # print('abnormal detection')\n",
    "                        sec_count +=1\n",
    "                        abnormal_count +=1\n",
    "                        continue\n",
    "                    cropped_faces = cv2.resize(cropped_faces, (resize, resize))\n",
    "                    write_path = output_path + f'{img_name}_{eye_coords[0]}_{i}.png'  # Use num_frame\n",
    "                    cv2.imwrite(write_path, cropped_faces)\n",
    "                    i += 1\n",
    "                    sec_count += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing frame {sec_count}: {e}\")\n",
    "                    sec_count += 1\n",
    "            else:\n",
    "                break\n",
    "\n",
    "\n",
    "        cap.release()\n",
    "        if abnormal_count!=0 or no_detection_count!=0:\n",
    "            print(last_substring)\n",
    "        if abnormal_count!=0:\n",
    "            print(f'abnormal detections: {abnormal_count}')\n",
    "        if no_detection_count!=0:\n",
    "            print(f'no detections: {no_detection_count}')\n",
    "        abnormal_total_count +=abnormal_count\n",
    "        no_detections_total_count +=no_detection_count\n",
    "    print(f'Total abnormal detections count: {abnormal_total_count}')\n",
    "    print(f'Total No detections count: {no_detections_total_count}')\n",
    "\n",
    "    return i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # START CAPTURING VIDEOS\n",
    "\n",
    "# def extract_multiple_videos(input_filenames,output_path,img_class,frame_rate, resize):\n",
    "#     \"\"\"Extract video files into sequence of images.\n",
    "#        input_filenames is a list for video file names\"\"\"\n",
    "\n",
    "#     i = 0  # Counter of first video\n",
    "\n",
    "#     # Iterate file names:\n",
    "#     for input_filename in input_filenames:\n",
    "        \n",
    "#         img_name = 'client'\n",
    "#         pattern = input_filename.split('/')\n",
    "#         last_substring = pattern[-1]  # Get the last substring after splitting by '/'\n",
    "#         print(last_substring)\n",
    "#         f_name = last_substring.split('_')  # Split the last substring by underscores\n",
    "#         if len(f_name) >= 4:  # Ensure there are at least 4 parts in the split\n",
    "#             img_name = '_'.join(f_name[:3])  # Join the first three parts with underscores\n",
    "    \n",
    "#         print(img_name)  # Print or use img_name as needed\n",
    "        \n",
    "#         # print('the image will be named as:', img_name)\n",
    "        \n",
    "        \n",
    "#         sec_count = 0\n",
    "#         cap = cv2.VideoCapture(input_filename)\n",
    "#         cap.set(cv2.CAP_PROP_POS_MSEC,(sec_count*frame_rate))\n",
    "\n",
    "#         # Keep iterating break\n",
    "#         while True:\n",
    "#             ret, frame = cap.read()  # Read frame from first video\n",
    "\n",
    "#             if ret:\n",
    "#                 try:\n",
    "#                     cropped_faces = extract_face(frame)\n",
    "#                     cropped_faces = cv2.resize(cropped_faces,(resize,resize))\n",
    "#                     write_path = output_path + f'{img_name}_{i}_{img_class}.png'\n",
    "#                     cv2.imwrite(write_path, cropped_faces)  # Write frame to JPEG file (1.jpg, 2.jpg, ...)\n",
    "#                     i += 1 # Advance file counter\n",
    "#                     sec_count += 1\n",
    "#                 except:\n",
    "#                     pass\n",
    "#             else:\n",
    "#                 # Break the interal loop when res status is False.\n",
    "#                 break\n",
    "\n",
    "#             delay = int(1000 / frame_rate)  # Calculate the delay based on the actual frame rate\n",
    "#             cv2.waitKey(delay)\n",
    "\n",
    "#         cap.release() #Release must be inside the outer loop\n",
    "#     return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from facenet_pytorch import MTCNN\n",
    "# import torch\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# mtcnn = MTCNN(keep_all=True, device=device)  # Initialize the detector outside the function\n",
    "\n",
    "# def extract_face_using_eyes(img, eye_coords):\n",
    "#     # Use eye coordinates to define region of interest (ROI) for face detection\n",
    "#     x_eye_left, y_eye_left, x_eye_right, y_eye_right = eye_coords\n",
    "    \n",
    "#     # Define the region around the eyes for face detection\n",
    "#     roi_x = min(x_eye_left, x_eye_right)\n",
    "#     roi_y = min(y_eye_left, y_eye_right)\n",
    "#     roi_w = max(x_eye_left, x_eye_right) - roi_x\n",
    "#     roi_h = max(y_eye_left, y_eye_right) - roi_y\n",
    "    \n",
    "#     # Extract the region for face detection\n",
    "#     roi = img[roi_y:roi_y+roi_h, roi_x:roi_x+roi_w]\n",
    "    \n",
    "#     # Detect faces in the ROI\n",
    "#     boxes, probs = mtcnn.detect(roi)\n",
    "    \n",
    "#     # Adjust the face coordinates to the original image space\n",
    "#     if boxes is not None:\n",
    "#         boxes[:, [0, 2]] += roi_x\n",
    "#         boxes[:, [1, 3]] += roi_y\n",
    "    \n",
    "#     return boxes\n",
    "\n",
    "# def extract_multiple_videos_with_eyes(input_filenames, output_path, img_class, frame_rate, resize):\n",
    "#     i = 0  # Counter of first video\n",
    "\n",
    "#     for input_filename in input_filenames:\n",
    "#         img_name = 'client'\n",
    "#         pattern = input_filename.split('/')\n",
    "#         last_substring = pattern[-1]\n",
    "#         f_name = last_substring.split('_')\n",
    "#         if len(f_name) >= 4:\n",
    "#             img_name = '_'.join(f_name[:3])\n",
    "    \n",
    "#         sec_count = 0\n",
    "#         cap = cv2.VideoCapture(input_filename)\n",
    "#         cap.set(cv2.CAP_PROP_POS_MSEC, (sec_count * frame_rate))\n",
    "\n",
    "#         while True:\n",
    "#             ret, frame = cap.read()\n",
    "\n",
    "#             if ret:\n",
    "#                 try:\n",
    "#                     # Read eye coordinates from the corresponding txt file\n",
    "#                     eye_coords_filename = f'{input_filename[:-4]}.txt'\n",
    "#                     with open(eye_coords_filename, 'r') as coords_file:\n",
    "#                         lines = coords_file.readlines()\n",
    "#                         if sec_count >= len(lines):\n",
    "#                             break  # Stop processing frames if no more coordinates are available\n",
    "#                         eye_coords = list(map(int, lines[sec_count].split(',')[1:]))\n",
    "\n",
    "#                     # Use eye coordinates for face detection around the eyes\n",
    "#                     face_results = extract_face_using_eyes(frame, eye_coords)\n",
    "\n",
    "#                     # Draw rectangles around detected faces\n",
    "#                     if face_results is not None:\n",
    "#                         for box in face_results:\n",
    "#                             x, y, w, h = box\n",
    "#                             cv2.rectangle(frame, (int(x), int(y)), (int(x + w), int(y + h)), (0, 255, 0), 2)\n",
    "\n",
    "#                     # Save the frame with rectangles\n",
    "#                     write_path = output_path + f'{img_name}_{i}_{img_class}.png'\n",
    "#                     cv2.imwrite(write_path, frame)\n",
    "\n",
    "#                     i += 1\n",
    "#                     sec_count += 1\n",
    "\n",
    "#                 except Exception as e:\n",
    "#                     print(f\"Error processing frame: {e}\")\n",
    "\n",
    "#             else:\n",
    "#                 break\n",
    "\n",
    "#             delay = int(1000 / frame_rate)\n",
    "#             cv2.waitKey(delay)\n",
    "\n",
    "#         cap.release()\n",
    "\n",
    "#     return i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start(in_dir , out_dir, img_class, frame_rate=32, resize=256):\n",
    "    filescounter = os.listdir(in_dir)\n",
    "    # Count the number of files\n",
    "    num_files = len(filescounter)\n",
    "    print(num_files)\n",
    "    \n",
    "    # traverse whole directory\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "    out_dir_real= os.path.join(out_dir, 'real/')\n",
    "    out_dir_attack= os.path.join(out_dir, 'attack/')\n",
    "    if not os.path.exists(out_dir_real):\n",
    "        os.makedirs(out_dir_real)\n",
    "    if not os.path.exists(out_dir_attack):\n",
    "        os.makedirs(out_dir_attack)\n",
    "    \n",
    "    real_dir_list = []\n",
    "    attack_dir_list = []\n",
    "    for root, dirs, files in os.walk(f'{in_dir}'):\n",
    "        # select file name\n",
    "        for file in files:\n",
    "            # check the extension of files\n",
    "            if file.endswith('.avi'):\n",
    "                # print whole path of files\n",
    "                base_name = os.path.splitext(file)[0]\n",
    "                if base_name[-1:] == '1':\n",
    "                    path = os.path.join(root, file)\n",
    "                    real_dir_list.append(path)\n",
    "                else:\n",
    "                    path = os.path.join(root, file)\n",
    "                    attack_dir_list.append(path)\n",
    "    count_real = extract_multiple_videos(real_dir_list,f'{out_dir_real}', resize=resize)\n",
    "    count_attack = extract_multiple_videos(attack_dir_list,f'{out_dir_attack}', resize=resize)\n",
    "    print(f'TOTAL FRAMES/IMAGES FORMED (REAL): {count_real}')\n",
    "    print(f'TOTAL FRAMES/IMAGES FORMED (ATTACK): {count_attack}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # START CAPTURING VIDEOS\n",
    "\n",
    "# def extract_attack_multiple_videos(input_filenames,output_path,img_class,frame_rate, resize):\n",
    "#     \"\"\"Extract video files into sequence of images.\n",
    "#        input_filenames is a list for video file names\"\"\"\n",
    "\n",
    "#     i = 0  # Counter of first video\n",
    "\n",
    "#     # Iterate file names:\n",
    "#     for input_filename in input_filenames:\n",
    "#         print(\"Processing video:\", input_filename)\n",
    "#         img_name = 'client'\n",
    "#         pattern = input_filename.split('/')\n",
    "#         for p in pattern:\n",
    "#             if 'client' in p:\n",
    "#                 f_name = p.split('_')\n",
    "#                 for name in f_name:\n",
    "#                     if name.find('client') != -1:\n",
    "#                         img_name = name\n",
    "        \n",
    "#         # print('the image will be named as:', img_name)\n",
    "        \n",
    "        \n",
    "#         sec_count = 0\n",
    "#         cap = cv2.VideoCapture(input_filename)\n",
    "#         cap.set(cv2.CAP_PROP_POS_MSEC,(sec_count*frame_rate))\n",
    "\n",
    "#         # Keep iterating break\n",
    "#         while True:\n",
    "#             ret, frame = cap.read()  # Read frame from first video\n",
    "#             if ret:\n",
    "#                 try:\n",
    "#                     cropped_faces = extract_face(frame)\n",
    "#                     cropped_faces = cv2.resize(cropped_faces,(resize,resize))\n",
    "#                     write_path = output_path + f'{img_name}_{i}_{img_class}.png'\n",
    "#                     cv2.imwrite(write_path, cropped_faces)  # Write frame to JPEG file (1.jpg, 2.jpg, ...)\n",
    "#                     i += 1 # Advance file counter\n",
    "#                     sec_count += 1\n",
    "#                 except:\n",
    "#                     pass\n",
    "#             else:\n",
    "#                 # Break the interal loop when res status is False.\n",
    "#                 break\n",
    "\n",
    "#             # cv2.waitKey(100) #Wait 100msec (for debugging)\n",
    "\n",
    "#         cap.release() #Release must be inside the outer loop\n",
    "#     return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def start_attack(in_dir , out_dir,img_class,frame_rate=32,resize=256):\n",
    "#     # traverse whole directory\n",
    "#     if not os.path.exists(out_dir):\n",
    "#         os.makedirs(out_dir)\n",
    "#     dir_list = []\n",
    "#     for root, dirs, files in os.walk(f'{in_dir}'):\n",
    "#         # select file name\n",
    "#         for file in files:\n",
    "#             # check the extension of files\n",
    "#             if file.endswith('.mov'):\n",
    "#                 # print whole path of files\n",
    "#                 path = os.path.join(root, file)\n",
    "#                 # print(path)\n",
    "#                 dir_list.append(path)\n",
    "#                 # extractImages(path,'./data/train/real',frame_rate=15000)\n",
    "#     count = extract_attack_multiple_videos(dir_list,f'{out_dir}',img_class,frame_rate=frame_rate, resize=resize)\n",
    "#     print(f'TOTAL FRAMES/IMAGES FORMED: {count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRANSFORMING TRAINING VIDEOS TO TRAINING IMAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vids_path = '/home/dinesh/Documents/Datasets/Face anti-spoofing datasets/OULU-NPU/Train_files'\n",
    "dev_vids_path = '/home/dinesh/Documents/Datasets/Face anti-spoofing datasets/OULU-NPU/Dev_files'\n",
    "test_vids_path = '/home/dinesh/Documents/Datasets/Face anti-spoofing datasets/OULU-NPU/Test_files'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imgs_path = '/home/dinesh/Documents/Datasets/Face anti-spoofing datasets/OULU-NPU-processed/train/'\n",
    "dev_imgs_path = '/home/dinesh/Documents/Datasets/Face anti-spoofing datasets/OULU-NPU-processed/dev/'\n",
    "test_imgs_path = '/home/dinesh/Documents/Datasets/Face anti-spoofing datasets/OULU-NPU-processed/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = {\"in_dir\": train_vids_path,\"out_dir\":train_imgs_path}\n",
    "dev_dir = {\"in_dir\":dev_vids_path, 'out_dir':dev_imgs_path}\n",
    "test_dir = {\"in_dir\":test_vids_path, 'out_dir':test_imgs_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3600\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29ed3d8cf7004e2cad74ef43212ab4a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Videos:   0%|          | 0/360 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dir\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43min_dir\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_dir\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mout_dir\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mimg_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mframe_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mresize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m)\u001b[49m \n",
      "Cell \u001b[0;32mIn[14], line 32\u001b[0m, in \u001b[0;36mstart\u001b[0;34m(in_dir, out_dir, img_class, frame_rate, resize)\u001b[0m\n\u001b[1;32m     30\u001b[0m                 path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root, file)\n\u001b[1;32m     31\u001b[0m                 attack_dir_list\u001b[38;5;241m.\u001b[39mappend(path)\n\u001b[0;32m---> 32\u001b[0m count_real \u001b[38;5;241m=\u001b[39m \u001b[43mextract_multiple_videos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreal_dir_list\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mout_dir_real\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m count_attack \u001b[38;5;241m=\u001b[39m extract_multiple_videos(attack_dir_list,\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_dir_attack\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, resize\u001b[38;5;241m=\u001b[39mresize)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTOTAL FRAMES/IMAGES FORMED (REAL): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcount_real\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[11], line 32\u001b[0m, in \u001b[0;36mextract_multiple_videos\u001b[0;34m(input_filenames, output_path, resize)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     31\u001b[0m     eye_coords \u001b[38;5;241m=\u001b[39m eye_coordinates[sec_count]\n\u001b[0;32m---> 32\u001b[0m     cropped_faces \u001b[38;5;241m=\u001b[39m \u001b[43mextract_face\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meye_coords\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Exclude num_frame\u001b[39;00m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cropped_faces \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;66;03m# print('No detection')\u001b[39;00m\n\u001b[1;32m     35\u001b[0m         sec_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[10], line 47\u001b[0m, in \u001b[0;36mextract_face\u001b[0;34m(img, eye_coords)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Try detecting with different parameters\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m face \u001b[38;5;241m=\u001b[39m \u001b[43mtry_detect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m face \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m face\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m face\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1e5\u001b[39m:\n\u001b[1;32m     49\u001b[0m     face \u001b[38;5;241m=\u001b[39m try_detect(\u001b[38;5;241m1.1\u001b[39m, \u001b[38;5;241m5\u001b[39m)\n",
      "Cell \u001b[0;32mIn[10], line 30\u001b[0m, in \u001b[0;36mextract_face.<locals>.try_detect\u001b[0;34m(scale_factor, min_neighbors)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtry_detect\u001b[39m(scale_factor, min_neighbors):\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 30\u001b[0m         faces \u001b[38;5;241m=\u001b[39m \u001b[43mface_cascade\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetectMultiScale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgray_eyes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_factor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_neighbors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(faces) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     32\u001b[0m             (fx, fy, fw, fh) \u001b[38;5;241m=\u001b[39m faces[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start(train_dir['in_dir'],train_dir['out_dir'],img_class=\"train\",frame_rate=30,resize=256) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "690863acf8ba44abbb813900121fcee7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Videos:   0%|          | 0/270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3_3_22_1.avi\n",
      "1_2_31_1.avi\n",
      "3_1_31_1.avi\n",
      "5_2_26_1.avi\n",
      "3_3_21_1.avi\n",
      "5_1_21_1.avi\n",
      "4_1_22_1.avi\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "3_3_35_1.avi\n",
      "2_3_34_1.avi\n",
      "1_2_21_1.avi\n",
      "3_3_26_1.avi\n",
      "3_2_27_1.avi\n",
      "6_2_31_1.avi\n",
      "2_3_27_1.avi\n",
      "3_3_30_1.avi\n",
      "5_1_27_1.avi\n",
      "1_2_23_1.avi\n",
      "2_3_32_1.avi\n",
      "2_1_27_1.avi\n",
      "2_3_28_1.avi\n",
      "3_2_28_1.avi\n",
      "6_1_23_1.avi\n",
      "3_3_32_1.avi\n",
      "6_1_32_1.avi\n",
      "2_3_21_1.avi\n",
      "4_3_28_1.avi\n",
      "2_1_33_1.avi\n",
      "4_1_21_1.avi\n",
      "5_2_32_1.avi\n",
      "2_1_35_1.avi\n",
      "1_3_34_1.avi\n",
      "5_1_24_1.avi\n",
      "5_3_30_1.avi\n",
      "2_1_34_1.avi\n",
      "4_3_25_1.avi\n",
      "4_2_34_1.avi\n",
      "5_1_29_1.avi\n",
      "2_3_31_1.avi\n",
      "5_1_32_1.avi\n",
      "3_1_25_1.avi\n",
      "1_1_25_1.avi\n",
      "5_1_30_1.avi\n",
      "6_2_35_1.avi\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdev_dir\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43min_dir\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdev_dir\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mout_dir\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mimg_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdev\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mframe_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mresize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m)\u001b[49m \n",
      "Cell \u001b[0;32mIn[14], line 32\u001b[0m, in \u001b[0;36mstart\u001b[0;34m(in_dir, out_dir, img_class, frame_rate, resize)\u001b[0m\n\u001b[1;32m     30\u001b[0m                 path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root, file)\n\u001b[1;32m     31\u001b[0m                 attack_dir_list\u001b[38;5;241m.\u001b[39mappend(path)\n\u001b[0;32m---> 32\u001b[0m count_real \u001b[38;5;241m=\u001b[39m \u001b[43mextract_multiple_videos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreal_dir_list\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mout_dir_real\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m count_attack \u001b[38;5;241m=\u001b[39m extract_multiple_videos(attack_dir_list,\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_dir_attack\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, resize\u001b[38;5;241m=\u001b[39mresize)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTOTAL FRAMES/IMAGES FORMED (REAL): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcount_real\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[11], line 29\u001b[0m, in \u001b[0;36mextract_multiple_videos\u001b[0;34m(input_filenames, output_path, resize)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     28\u001b[0m     eye_coords \u001b[38;5;241m=\u001b[39m eye_coordinates[sec_count]\n\u001b[0;32m---> 29\u001b[0m     cropped_faces \u001b[38;5;241m=\u001b[39m \u001b[43mextract_face\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meye_coords\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Exclude num_frame\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cropped_faces \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     31\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo detection\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[10], line 47\u001b[0m, in \u001b[0;36mextract_face\u001b[0;34m(img, eye_coords)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Try detecting with different parameters\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m face \u001b[38;5;241m=\u001b[39m \u001b[43mtry_detect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m face \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m face\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m face\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1e5\u001b[39m:\n\u001b[1;32m     49\u001b[0m     face \u001b[38;5;241m=\u001b[39m try_detect(\u001b[38;5;241m1.1\u001b[39m, \u001b[38;5;241m5\u001b[39m)\n",
      "Cell \u001b[0;32mIn[10], line 30\u001b[0m, in \u001b[0;36mextract_face.<locals>.try_detect\u001b[0;34m(scale_factor, min_neighbors)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtry_detect\u001b[39m(scale_factor, min_neighbors):\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 30\u001b[0m         faces \u001b[38;5;241m=\u001b[39m \u001b[43mface_cascade\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetectMultiScale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgray_eyes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_factor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_neighbors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(faces) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     32\u001b[0m             (fx, fy, fw, fh) \u001b[38;5;241m=\u001b[39m faces[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start(dev_dir['in_dir'],dev_dir['out_dir'],img_class=\"dev\",frame_rate=32,resize=256) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3600\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2161a61af1d849f3a07e29526e65ac8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Videos:   0%|          | 0/360 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1_3_46_1.avi\n",
      "4_3_43_1.avi\n",
      "6_2_47_1.avi\n",
      "3_2_45_1.avi\n",
      "3_1_44_1.avi\n",
      "5_2_53_1.avi\n",
      "1_1_48_1.avi\n",
      "3_2_51_1.avi\n",
      "5_2_51_1.avi\n",
      "3_3_39_1.avi\n",
      "5_3_54_1.avi\n",
      "4_3_46_1.avi\n",
      "5_3_44_1.avi\n",
      "No detection\n",
      "1_1_55_1.avi\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "abnormal detection\n",
      "1_2_55_1.avi\n",
      "3_1_37_1.avi\n",
      "6_1_53_1.avi\n",
      "4_1_37_1.avi\n",
      "1_2_38_1.avi\n",
      "6_3_36_1.avi\n",
      "2_3_40_1.avi\n",
      "4_1_47_1.avi\n",
      "1_3_49_1.avi\n",
      "1_2_49_1.avi\n",
      "5_1_42_1.avi\n",
      "4_2_39_1.avi\n",
      "6_3_51_1.avi\n",
      "6_3_54_1.avi\n",
      "5_1_37_1.avi\n",
      "5_1_53_1.avi\n",
      "3_2_37_1.avi\n",
      "6_1_37_1.avi\n",
      "6_3_39_1.avi\n",
      "2_2_41_1.avi\n",
      "4_1_54_1.avi\n",
      "3_3_42_1.avi\n",
      "2_3_37_1.avi\n",
      "5_3_45_1.avi\n",
      "6_2_46_1.avi\n",
      "2_1_55_1.avi\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_dir\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43min_dir\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtest_dir\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mout_dir\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mimg_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mframe_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mresize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 32\u001b[0m, in \u001b[0;36mstart\u001b[0;34m(in_dir, out_dir, img_class, frame_rate, resize)\u001b[0m\n\u001b[1;32m     30\u001b[0m                 path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root, file)\n\u001b[1;32m     31\u001b[0m                 attack_dir_list\u001b[38;5;241m.\u001b[39mappend(path)\n\u001b[0;32m---> 32\u001b[0m count_real \u001b[38;5;241m=\u001b[39m \u001b[43mextract_multiple_videos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreal_dir_list\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mout_dir_real\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m count_attack \u001b[38;5;241m=\u001b[39m extract_multiple_videos(attack_dir_list,\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_dir_attack\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, resize\u001b[38;5;241m=\u001b[39mresize)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTOTAL FRAMES/IMAGES FORMED (REAL): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcount_real\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[11], line 29\u001b[0m, in \u001b[0;36mextract_multiple_videos\u001b[0;34m(input_filenames, output_path, resize)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     28\u001b[0m     eye_coords \u001b[38;5;241m=\u001b[39m eye_coordinates[sec_count]\n\u001b[0;32m---> 29\u001b[0m     cropped_faces \u001b[38;5;241m=\u001b[39m \u001b[43mextract_face\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meye_coords\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Exclude num_frame\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cropped_faces \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     31\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo detection\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[10], line 47\u001b[0m, in \u001b[0;36mextract_face\u001b[0;34m(img, eye_coords)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Try detecting with different parameters\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m face \u001b[38;5;241m=\u001b[39m \u001b[43mtry_detect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m face \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m face\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m face\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1e5\u001b[39m:\n\u001b[1;32m     49\u001b[0m     face \u001b[38;5;241m=\u001b[39m try_detect(\u001b[38;5;241m1.1\u001b[39m, \u001b[38;5;241m5\u001b[39m)\n",
      "Cell \u001b[0;32mIn[10], line 30\u001b[0m, in \u001b[0;36mextract_face.<locals>.try_detect\u001b[0;34m(scale_factor, min_neighbors)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtry_detect\u001b[39m(scale_factor, min_neighbors):\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 30\u001b[0m         faces \u001b[38;5;241m=\u001b[39m \u001b[43mface_cascade\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetectMultiScale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgray_eyes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_factor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_neighbors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(faces) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     32\u001b[0m             (fx, fy, fw, fh) \u001b[38;5;241m=\u001b[39m faces[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start(test_dir['in_dir'],test_dir['out_dir'],img_class=\"test\",frame_rate=32,resize=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRANSFORMING TEST VIDEOS TO TESTING IMAGES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_real_vids_path = 'D:/BTP/dataset/Replay-Attack/test/real/'\n",
    "test_spoof_fixed_vids_path = 'D:/BTP/dataset/Replay-Attack/test/attack/fixed/'\n",
    "test_spoof_hand_vids_path = 'D:/BTP/dataset/Replay-Attack/test/attack/hand/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_real_imgs_path = './data/Replay-Attack-processed/test/real/'\n",
    "test_spoof_fixed_imgs_path = './data/Replay-Attack-processed/test/spoof/fixed/'\n",
    "test_spoof_hand_imgs_path = './data/Replay-Attack-processed/test/spoof/hand/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir_real = {\"in_dir\": test_real_vids_path,\"out_dir\":test_real_imgs_path}\n",
    "test_dir_attack_fixed = {\"in_dir\":test_spoof_fixed_vids_path, 'out_dir':test_spoof_fixed_imgs_path}\n",
    "test_dir_attack_hand = {\"in_dir\":test_spoof_hand_vids_path, 'out_dir':test_spoof_hand_imgs_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL FRAMES/IMAGES FORMED: 29229\n"
     ]
    }
   ],
   "source": [
    "start(test_dir_real['in_dir'],test_dir_real['out_dir'],img_class=\"real\",frame_rate=2000,resize=256) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL FRAMES/IMAGES FORMED: 46700\n"
     ]
    }
   ],
   "source": [
    "start(test_dir_attack_fixed['in_dir'],test_dir_attack_fixed['out_dir'],img_class=\"attack_fixed\",frame_rate=500,resize=256) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL FRAMES/IMAGES FORMED: 44487\n"
     ]
    }
   ],
   "source": [
    "start(test_dir_attack_hand['in_dir'],test_dir_attack_hand['out_dir'],img_class=\"attack_hand\",frame_rate=500,resize=256) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data saved to train_data.csv\n",
      "Test data saved to test_data.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import csv\n",
    "\n",
    "# Define the paths to your train and test folders\n",
    "train_folder = './data/Replay-Attack-processed/train/'\n",
    "test_folder = './data/Replay-Attack-processed/test/'\n",
    "\n",
    "# Function to label and shuffle data in a folder\n",
    "def process_folder(folder_path, label):\n",
    "    image_paths = []\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.png'):\n",
    "                image_path = os.path.join(root, file)\n",
    "                image_paths.append((image_path, label))\n",
    "    \n",
    "    # Shuffle the image paths\n",
    "    random.shuffle(image_paths)\n",
    "    \n",
    "    return image_paths\n",
    "\n",
    "# Process the train and test folders\n",
    "train_real_paths = process_folder(os.path.join(train_folder, 'real'), 1)\n",
    "test_real_paths = process_folder(os.path.join(test_folder, 'real'), 1)\n",
    "\n",
    "# Include images from the \"hand\" and \"fixed\" subfolders within the \"spoof\" folder\n",
    "train_fake_paths = process_folder(os.path.join(train_folder, 'spoof/hand'), 0)\n",
    "train_fake_paths += process_folder(os.path.join(train_folder, 'spoof/fixed'), 0)\n",
    "\n",
    "test_fake_paths = process_folder(os.path.join(test_folder, 'spoof/hand'), 0)\n",
    "test_fake_paths += process_folder(os.path.join(test_folder, 'spoof/fixed'), 0)\n",
    "\n",
    "# Combine real and fake paths\n",
    "train_data = train_real_paths + train_fake_paths\n",
    "test_data = test_real_paths + test_fake_paths\n",
    "\n",
    "# Shuffle the combined data\n",
    "random.shuffle(train_data)\n",
    "random.shuffle(test_data)\n",
    "\n",
    "# Define CSV filenames\n",
    "train_csv_filename = 'train_data.csv'\n",
    "test_csv_filename = 'test_data.csv'\n",
    "\n",
    "# Write data to CSV files\n",
    "def write_to_csv(filename, data):\n",
    "    with open(filename, 'w', newline='') as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        csv_writer.writerow(['name', 'label'])\n",
    "        csv_writer.writerows(data)\n",
    "\n",
    "write_to_csv(train_csv_filename, train_data)\n",
    "write_to_csv(test_csv_filename, test_data)\n",
    "\n",
    "print(f\"Train data saved to {train_csv_filename}\")\n",
    "print(f\"Test data saved to {test_csv_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CHANGE DIRECTORY STRUCTURE BY PLACING IMAGES PERSON WISE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_wise_dataset_train_path = './data/Reply-Attack-processed/train/images/' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dir_list(path):\n",
    "    dir_list = []\n",
    "    for root, dirs, files in os.walk(f'{path}'):\n",
    "        # select file name\n",
    "        for file in files:\n",
    "            # check the extension of files\n",
    "            if file.endswith('.png'):\n",
    "                # print whole path of files\n",
    "                path = os.path.join(root, file)\n",
    "                # print(path)\n",
    "                dir_list.append(path)\n",
    "    return dir_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22490\n",
      "35438\n",
      "33707\n"
     ]
    }
   ],
   "source": [
    "img_path_list_train_real = get_dir_list(train_real_imgs_path)\n",
    "img_path_list_train_spoof_fixed = get_dir_list(train_spoof_fixed_imgs_path)\n",
    "img_path_list_train_spoof_hand = get_dir_list(train_spoof_hand_imgs_path)\n",
    "print(len(img_path_list_train_real)) # ./replay-images/train/real/\n",
    "print(len(img_path_list_train_spoof_fixed)) # ./replay-images/train/spoof/fixed\n",
    "print(len(img_path_list_train_spoof_hand)) # ./replay-images/train/spoof/hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/Reply-Attack-processed/train/real/client001_0_real.png'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_path_list_train_real[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_client_objects(path_list):\n",
    "    clients = {}\n",
    "    for file in path_list:\n",
    "        path_parts = file.split('/')\n",
    "        for part in path_parts:\n",
    "            if 'client' in part:\n",
    "                file_name_split = part.split('_')\n",
    "                client_key = file_name_split[0]\n",
    "                if client_key in clients.keys():\n",
    "                    clients[client_key].append(file)\n",
    "                else:\n",
    "                    clients[client_key] = [file]\n",
    "    return clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_client_objects =  get_client_objects(img_path_list_train_real)\n",
    "spoof_fixed_client_objects = get_client_objects(img_path_list_train_spoof_fixed)\n",
    "spoof_hand_client_objects = get_client_objects(img_path_list_train_spoof_hand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['client001', 'client002', 'client004', 'client006', 'client007', 'client008', 'client012', 'client016', 'client018', 'client025', 'client027', 'client103', 'client105', 'client108', 'client110']\n",
      "['client001', 'client002', 'client004', 'client006', 'client007', 'client008', 'client012', 'client016', 'client018', 'client025', 'client027', 'client103', 'client105', 'client108', 'client110']\n",
      "['client001', 'client002', 'client004', 'client006', 'client007', 'client008', 'client012', 'client016', 'client018', 'client025', 'client027', 'client103', 'client105', 'client108', 'client110']\n"
     ]
    }
   ],
   "source": [
    "print(sorted(real_client_objects.keys()))\n",
    "print(sorted(spoof_fixed_client_objects.keys()))\n",
    "print(sorted(spoof_hand_client_objects.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_wise_dir_list = get_dir_list(person_wise_dataset_train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "person_wise_dir_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " DataGeneration.ipynb                              \u001b[0m\u001b[01;34mreplay-dummy-person-wise\u001b[0m/\r\n",
      " \u001b[01;34mdataset\u001b[0m/                                          \u001b[01;34mreplay-images\u001b[0m/\r\n",
      " haarcascade_frontalface_default.xml               \u001b[01;35mtest_img.png\u001b[0m\r\n",
      "'PIxel Wise Supervision - Siamese-Network.ipynb'\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATE list.txt AND APPEND FORMATED PATHS OF IMAGES TO IT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/Reply-Attack-processed/train/list.txt', 'w') as f:\n",
    "    for path in person_wise_dir_list:\n",
    "        if '/data/Reply-Attack-processed/train/images/' in path:\n",
    "            s = path.split('/data/Reply-Attack-processed/train/images/')\n",
    "            p = s[1]\n",
    "            f.write(\"%s\\n\" % p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: './data/Reply-Attack-processed/train/images/client001'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\BTP\\maincode\\replay-attack-processing.ipynb Cell 41\u001b[0m in \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/BTP/maincode/replay-attack-processing.ipynb#X45sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     shutil\u001b[39m.\u001b[39mrmtree(person_wise_dataset_train_path\u001b[39m+\u001b[39mclient_key)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/BTP/maincode/replay-attack-processing.ipynb#X45sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m single_client_path \u001b[39m=\u001b[39m person_wise_dataset_train_path\u001b[39m+\u001b[39mclient_key\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/BTP/maincode/replay-attack-processing.ipynb#X45sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m os\u001b[39m.\u001b[39;49mmkdir(single_client_path)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/BTP/maincode/replay-attack-processing.ipynb#X45sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m single_client_path_real \u001b[39m=\u001b[39m single_client_path \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/real\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/BTP/maincode/replay-attack-processing.ipynb#X45sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m single_client_path_spoof \u001b[39m=\u001b[39m single_client_path \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/spoof\u001b[39m\u001b[39m'\u001b[39m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: './data/Reply-Attack-processed/train/images/client001'"
     ]
    }
   ],
   "source": [
    "for client_key in sorted(real_client_objects.keys()):\n",
    "    if os.path.isdir(person_wise_dataset_train_path+client_key):\n",
    "        shutil.rmtree(person_wise_dataset_train_path+client_key)\n",
    "    single_client_path = person_wise_dataset_train_path+client_key\n",
    "    os.mkdir(single_client_path)\n",
    "    \n",
    "    single_client_path_real = single_client_path + '/real'\n",
    "    single_client_path_spoof = single_client_path + '/spoof'\n",
    "    os.mkdir(single_client_path_real)\n",
    "    os.mkdir(single_client_path_spoof)\n",
    "    for raw_client_image_path in real_client_objects[client_key]:\n",
    "        shutil.copy2(raw_client_image_path,single_client_path_real)\n",
    "    for raw_client_image_path in spoof_fixed_client_objects[client_key]:\n",
    "        shutil.copy2(raw_client_image_path,single_client_path_spoof)\n",
    "    for raw_client_image_path in spoof_hand_client_objects[client_key]:\n",
    "        shutil.copy2(raw_client_image_path,single_client_path_spoof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchKernel",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
